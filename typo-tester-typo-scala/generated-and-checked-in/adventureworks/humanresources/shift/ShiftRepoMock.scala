/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks.humanresources.shift

import java.lang.RuntimeException
import java.sql.Connection
import java.util.ArrayList
import java.util.HashMap
import java.util.Optional
import java.util.function.Function
import java.util.stream.Collectors
import typo.dsl.DeleteBuilder
import typo.dsl.DeleteBuilder.DeleteBuilderMock
import typo.dsl.DeleteParams
import typo.dsl.SelectBuilder
import typo.dsl.SelectBuilderMock
import typo.dsl.SelectParams
import typo.dsl.UpdateBuilder
import typo.dsl.UpdateBuilder.UpdateBuilderMock
import typo.dsl.UpdateParams

case class ShiftRepoMock(
  toRow: ShiftRowUnsaved => ShiftRow,
  map: HashMap[ShiftId, ShiftRow] = new HashMap[ShiftId, ShiftRow]()
) extends ShiftRepo {
  override def delete: DeleteBuilder[ShiftFields, ShiftRow] = {
    new DeleteBuilderMock(
      ShiftFields.structure,
      () => new ArrayList(map.values()),
      DeleteParams.empty(),
      row => row.shiftid,
      id => map.remove(id): @scala.annotation.nowarn
    )
  }

  override def deleteById(shiftid: ShiftId)(using c: Connection): java.lang.Boolean = Optional.ofNullable(map.remove(shiftid)).isPresent()

  override def deleteByIds(shiftids: Array[ShiftId])(using c: Connection): Integer = {
    var count = 0
    shiftids.foreach { id => if (Optional.ofNullable(map.remove(id)).isPresent()) {
      count = count + 1
    } }
    return count
  }

  override def insert(unsaved: ShiftRow)(using c: Connection): ShiftRow = {
    if (map.containsKey(unsaved.shiftid)) {
      throw new RuntimeException(s"id $unsaved.shiftid already exists")
    }
    map.put(unsaved.shiftid, unsaved): @scala.annotation.nowarn
    return unsaved
  }

  override def insert(unsaved: ShiftRowUnsaved)(using c: Connection): ShiftRow = insert(toRow(unsaved))(using c)

  override def insertStreaming(
    unsaved: java.util.Iterator[ShiftRow],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1L
    }
    return count
  }

  /** NOTE: this functionality requires PostgreSQL 16 or later! */
  override def insertUnsavedStreaming(
    unsaved: java.util.Iterator[ShiftRowUnsaved],
    batchSize: Integer = 10000
  )(using c: Connection): java.lang.Long = {
    var count = 0L
    while (unsaved.hasNext()) {
      val unsavedRow = unsaved.next()
      val row = toRow(unsavedRow)
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1L
    }
    return count
  }

  override def select: SelectBuilder[ShiftFields, ShiftRow] = new SelectBuilderMock(ShiftFields.structure, () => new ArrayList(map.values()), SelectParams.empty())

  override def selectAll(using c: Connection): java.util.List[ShiftRow] = new ArrayList(map.values())

  override def selectById(shiftid: ShiftId)(using c: Connection): Optional[ShiftRow] = Optional.ofNullable(map.get(shiftid))

  override def selectByIds(shiftids: Array[ShiftId])(using c: Connection): java.util.List[ShiftRow] = {
    val result = new ArrayList[ShiftRow]()
    shiftids.foreach { id => val opt = Optional.ofNullable(map.get(id)); if (opt.isPresent()) {
      result.add(opt.get()): @scala.annotation.nowarn
    } }
    return result
  }

  override def selectByIdsTracked(shiftids: Array[ShiftId])(using c: Connection): java.util.Map[ShiftId, ShiftRow] = selectByIds(shiftids)(using c).stream().collect(Collectors.toMap((row: ShiftRow) => row.shiftid, Function.identity()))

  override def update: UpdateBuilder[ShiftFields, ShiftRow] = {
    new UpdateBuilderMock(
      ShiftFields.structure,
      () => new ArrayList(map.values()),
      UpdateParams.empty(),
      row => row
    )
  }

  override def update(row: ShiftRow)(using c: Connection): java.lang.Boolean = {
    val shouldUpdate = Optional.ofNullable(map.get(row.shiftid)).filter(oldRow => (oldRow != row)).isPresent()
    if (shouldUpdate) {
      map.put(row.shiftid, row): @scala.annotation.nowarn
    }
    return shouldUpdate
  }

  override def upsert(unsaved: ShiftRow)(using c: Connection): ShiftRow = {
    map.put(unsaved.shiftid, unsaved): @scala.annotation.nowarn
    return unsaved
  }

  override def upsertBatch(unsaved: java.util.Iterator[ShiftRow])(using c: Connection): java.util.List[ShiftRow] = {
    val result = new ArrayList[ShiftRow]()
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      result.add(row): @scala.annotation.nowarn
    }
    return result
  }

  /** NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  override def upsertStreaming(
    unsaved: java.util.Iterator[ShiftRow],
    batchSize: Integer = 10000
  )(using c: Connection): Integer = {
    var count = 0
    while (unsaved.hasNext()) {
      val row = unsaved.next()
      map.put(row.shiftid, row): @scala.annotation.nowarn
      count = count + 1
    }
    return count
  }
}