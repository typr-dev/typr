/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks.public.precision_types

import doobie.free.connection.ConnectionIO
import doobie.free.connection.delay
import fs2.Stream
import scala.annotation.nowarn
import typr.dsl.DeleteBuilder
import typr.dsl.DeleteBuilderMock
import typr.dsl.DeleteParams
import typr.dsl.SelectBuilder
import typr.dsl.SelectBuilderMock
import typr.dsl.SelectParams
import typr.dsl.UpdateBuilder
import typr.dsl.UpdateBuilderMock
import typr.dsl.UpdateParams

case class PrecisionTypesRepoMock(
  toRow: PrecisionTypesRowUnsaved => PrecisionTypesRow,
  map: scala.collection.mutable.Map[PrecisionTypesId, PrecisionTypesRow] = scala.collection.mutable.Map.empty[PrecisionTypesId, PrecisionTypesRow]
) extends PrecisionTypesRepo {
  override def delete: DeleteBuilder[PrecisionTypesFields, PrecisionTypesRow] = DeleteBuilderMock(DeleteParams.empty, PrecisionTypesFields.structure, map)

  override def deleteById(id: PrecisionTypesId): ConnectionIO[Boolean] = delay(map.remove(id).isDefined)

  override def deleteByIds(ids: Array[PrecisionTypesId]): ConnectionIO[Int] = delay(ids.map(id => map.remove(id)).count(_.isDefined))

  override def insert(unsaved: PrecisionTypesRow): ConnectionIO[PrecisionTypesRow] = {
  delay {
    val _ = if (map.contains(unsaved.id))
      sys.error(s"id ${unsaved.id} already exists")
    else
      map.put(unsaved.id, unsaved)

    unsaved
  }
  }

  override def insert(unsaved: PrecisionTypesRowUnsaved): ConnectionIO[PrecisionTypesRow] = insert(toRow(unsaved))

  override def insertStreaming(
    unsaved: Stream[ConnectionIO, PrecisionTypesRow],
    batchSize: Int = 10000
  ): ConnectionIO[Long] = {
    unsaved.compile.toList.map { rows =>
      var num = 0L
      rows.foreach { row =>
        map += (row.id -> row)
        num += 1
      }
      num
    }
  }

  /** NOTE: this functionality requires PostgreSQL 16 or later! */
  override def insertUnsavedStreaming(
    unsaved: Stream[ConnectionIO, PrecisionTypesRowUnsaved],
    batchSize: Int = 10000
  ): ConnectionIO[Long] = {
    unsaved.compile.toList.map { unsavedRows =>
      var num = 0L
      unsavedRows.foreach { unsavedRow =>
        val row = toRow(unsavedRow)
        map += (row.id -> row)
        num += 1
      }
      num
    }
  }

  override def select: SelectBuilder[PrecisionTypesFields, PrecisionTypesRow] = SelectBuilderMock(PrecisionTypesFields.structure, delay(map.values.toList), SelectParams.empty)

  override def selectAll: Stream[ConnectionIO, PrecisionTypesRow] = Stream.emits(map.values.toList)

  override def selectById(id: PrecisionTypesId): ConnectionIO[Option[PrecisionTypesRow]] = delay(map.get(id))

  override def selectByIds(ids: Array[PrecisionTypesId]): Stream[ConnectionIO, PrecisionTypesRow] = Stream.emits(ids.flatMap(map.get).toList)

  override def selectByIdsTracked(ids: Array[PrecisionTypesId]): ConnectionIO[Map[PrecisionTypesId, PrecisionTypesRow]] = {
    selectByIds(ids).compile.toList.map { rows =>
      val byId = rows.view.map(x => (x.id, x)).toMap
      ids.view.flatMap(id => byId.get(id).map(x => (id, x))).toMap
    }
  }

  override def update: UpdateBuilder[PrecisionTypesFields, PrecisionTypesRow] = UpdateBuilderMock(UpdateParams.empty, PrecisionTypesFields.structure, map)

  override def update(row: PrecisionTypesRow): ConnectionIO[Option[PrecisionTypesRow]] = {
    delay {
      map.get(row.id).map { _ =>
        map.put(row.id, row): @nowarn
        row
      }
    }
  }

  override def upsert(unsaved: PrecisionTypesRow): ConnectionIO[PrecisionTypesRow] = {
    delay {
      map.put(unsaved.id, unsaved): @nowarn
      unsaved
    }
  }

  override def upsertBatch(unsaved: List[PrecisionTypesRow]): Stream[ConnectionIO, PrecisionTypesRow] = {
    Stream.emits {
      unsaved.map { row =>
        map += (row.id -> row)
        row
      }
    }
  }

  /** NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  override def upsertStreaming(
    unsaved: Stream[ConnectionIO, PrecisionTypesRow],
    batchSize: Int = 10000
  ): ConnectionIO[Int] = {
    unsaved.compile.toList.map { rows =>
      var num = 0
      rows.foreach { row =>
        map += (row.id -> row)
        num += 1
      }
      num
    }
  }
}