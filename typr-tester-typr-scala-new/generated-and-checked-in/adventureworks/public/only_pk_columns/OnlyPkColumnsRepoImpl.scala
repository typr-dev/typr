/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks.public.only_pk_columns

import java.sql.Connection
import typr.runtime.PgTypes
import typr.runtime.streamingInsert
import typr.scaladsl.DeleteBuilder
import typr.scaladsl.Dialect
import typr.scaladsl.Fragment
import typr.scaladsl.ScalaDbTypes
import typr.scaladsl.ScalaIteratorOps
import typr.scaladsl.SelectBuilder
import typr.scaladsl.UpdateBuilder
import typr.scaladsl.Fragment.sql

class OnlyPkColumnsRepoImpl extends OnlyPkColumnsRepo {
  override def delete: DeleteBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = DeleteBuilder.of(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, Dialect.POSTGRESQL)

  override def deleteById(compositeId: OnlyPkColumnsId)(using c: Connection): Boolean = sql"""delete from "public"."only_pk_columns" where "key_column_1" = ${Fragment.encode(PgTypes.text, compositeId.keyColumn1)} AND "key_column_2" = ${Fragment.encode(ScalaDbTypes.PgTypes.int4, compositeId.keyColumn2)}""".update().runUnchecked(c) > 0

  override def deleteByIds(compositeIds: Array[OnlyPkColumnsId])(using c: Connection): Int = {
    val keyColumn1: Array[String] = compositeIds.map(_.keyColumn1)
    val keyColumn2: Array[Int] = compositeIds.map(_.keyColumn2)
    return sql"""delete
    from "public"."only_pk_columns"
    where ("key_column_1", "key_column_2")
    in (select * from unnest(${Fragment.encode(PgTypes.textArray, keyColumn1)}, ${Fragment.encode(PgTypes.int4ArrayUnboxed, keyColumn2)}))
    """.update().runUnchecked(c)
  }

  override def insert(unsaved: OnlyPkColumnsRow)(using c: Connection): OnlyPkColumnsRow = {
  sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
    values (${Fragment.encode(PgTypes.text, unsaved.keyColumn1)}, ${Fragment.encode(ScalaDbTypes.PgTypes.int4, unsaved.keyColumn2)}::int4)
    RETURNING "key_column_1", "key_column_2"
    """
    .updateReturning(OnlyPkColumnsRow.`_rowParser`.exactlyOne()).runUnchecked(c)
  }

  override def insertStreaming(
    unsaved: Iterator[OnlyPkColumnsRow],
    batchSize: Int = 10000
  )(using c: Connection): Long = streamingInsert.insertUnchecked(s"""COPY "public"."only_pk_columns"("key_column_1", "key_column_2") FROM STDIN""", batchSize, unsaved.toJavaIterator, c, OnlyPkColumnsRow.pgText)

  override def select: SelectBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = SelectBuilder.of(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.`_rowParser`, Dialect.POSTGRESQL)

  override def selectAll(using c: Connection): List[OnlyPkColumnsRow] = {
    sql"""select "key_column_1", "key_column_2"
    from "public"."only_pk_columns"
    """.query(OnlyPkColumnsRow.`_rowParser`.all()).runUnchecked(c)
  }

  override def selectById(compositeId: OnlyPkColumnsId)(using c: Connection): Option[OnlyPkColumnsRow] = {
    sql"""select "key_column_1", "key_column_2"
    from "public"."only_pk_columns"
    where "key_column_1" = ${Fragment.encode(PgTypes.text, compositeId.keyColumn1)} AND "key_column_2" = ${Fragment.encode(ScalaDbTypes.PgTypes.int4, compositeId.keyColumn2)}""".query(OnlyPkColumnsRow.`_rowParser`.first()).runUnchecked(c)
  }

  override def selectByIds(compositeIds: Array[OnlyPkColumnsId])(using c: Connection): List[OnlyPkColumnsRow] = {
    val keyColumn1: Array[String] = compositeIds.map(_.keyColumn1)
    val keyColumn2: Array[Int] = compositeIds.map(_.keyColumn2)
    return sql"""select "key_column_1", "key_column_2"
    from "public"."only_pk_columns"
    where ("key_column_1", "key_column_2")
    in (select * from unnest(${Fragment.encode(PgTypes.textArray, keyColumn1)}, ${Fragment.encode(PgTypes.int4ArrayUnboxed, keyColumn2)}))
    """.query(OnlyPkColumnsRow.`_rowParser`.all()).runUnchecked(c)
  }

  override def selectByIdsTracked(compositeIds: Array[OnlyPkColumnsId])(using c: Connection): Map[OnlyPkColumnsId, OnlyPkColumnsRow] = {
    val ret: scala.collection.mutable.Map[OnlyPkColumnsId, OnlyPkColumnsRow] = scala.collection.mutable.Map.empty[OnlyPkColumnsId, OnlyPkColumnsRow]
    selectByIds(compositeIds)(using c).foreach(row => ret.put(row.compositeId, row): @scala.annotation.nowarn)
    return ret.toMap
  }

  override def update: UpdateBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = UpdateBuilder.of(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.`_rowParser`, Dialect.POSTGRESQL)

  override def upsert(unsaved: OnlyPkColumnsRow)(using c: Connection): OnlyPkColumnsRow = {
  sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
    values (${Fragment.encode(PgTypes.text, unsaved.keyColumn1)}, ${Fragment.encode(ScalaDbTypes.PgTypes.int4, unsaved.keyColumn2)}::int4)
    on conflict ("key_column_1", "key_column_2")
    do update set "key_column_1" = EXCLUDED."key_column_1"
    returning "key_column_1", "key_column_2""""
    .updateReturning(OnlyPkColumnsRow.`_rowParser`.exactlyOne())
    .runUnchecked(c)
  }

  override def upsertBatch(unsaved: Iterator[OnlyPkColumnsRow])(using c: Connection): List[OnlyPkColumnsRow] = {
    sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
    values (?, ?::int4)
    on conflict ("key_column_1", "key_column_2")
    do update set "key_column_1" = EXCLUDED."key_column_1"
    returning "key_column_1", "key_column_2""""
      .updateManyReturning(OnlyPkColumnsRow.`_rowParser`, unsaved)
    .runUnchecked(c)
  }

  /** NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  override def upsertStreaming(
    unsaved: Iterator[OnlyPkColumnsRow],
    batchSize: Int = 10000
  )(using c: Connection): Int = {
    sql"""create temporary table only_pk_columns_TEMP (like "public"."only_pk_columns") on commit drop""".update().runUnchecked(c): @scala.annotation.nowarn
    streamingInsert.insertUnchecked(s"""copy only_pk_columns_TEMP("key_column_1", "key_column_2") from stdin""", batchSize, unsaved.toJavaIterator, c, OnlyPkColumnsRow.pgText): @scala.annotation.nowarn
    return sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
    select * from only_pk_columns_TEMP
    on conflict ("key_column_1", "key_column_2")
    do nothing
    ;
    drop table only_pk_columns_TEMP;""".update().runUnchecked(c)
  }
}